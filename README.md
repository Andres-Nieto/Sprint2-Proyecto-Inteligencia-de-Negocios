# ğŸ“ **ICFES Analytics â€“ Proyecto de Inteligencia de Negocios**
Sistema completo de anÃ¡lisis, clusterizaciÃ³n, predicciÃ³n y recomendaciones acadÃ©micas basado en datos de **ICFES Saber 11** y **Saber Pro**, implementado en Python mediante un pipeline profesional de *Machine Learning + API + Dashboard*.

## ğŸ‘¥ Autores
- Oscar Daniel Casallas Lozano â€“ 2220221011
- AndrÃ©s Fernando Nieto ... - 
- David Santiago Manchola Serna - 2220221093
- 
---

# ğŸ“‘ **Tabla de Contenidos**
- [ğŸ“ **ICFES Analytics â€“ Proyecto de Inteligencia de Negocios**](#-icfes-analytics--proyecto-de-inteligencia-de-negocios)
  - [ğŸ‘¥ Autores](#-autores)
- [ğŸ“‘ **Tabla de Contenidos**](#-tabla-de-contenidos)
- [ğŸ¯ **DescripciÃ³n del Proyecto**](#-descripciÃ³n-del-proyecto)
- [ğŸ“‚ **Datasets**](#-datasets)
    - [ğŸ“Œ **Fuente**](#-fuente)
    - [ğŸ“¥ Descarga](#-descarga)
    - [ğŸ“ UbicaciÃ³n esperada](#-ubicaciÃ³n-esperada)
- [ğŸ—ï¸ **Arquitectura del Proyecto**](#ï¸-arquitectura-del-proyecto)
- [ğŸ§° **Requisitos Previos**](#-requisitos-previos)
    - [âœ”ï¸ Software necesario](#ï¸-software-necesario)
    - [âœ”ï¸ Verificar versiones](#ï¸-verificar-versiones)
- [âš™ï¸ InstalaciÃ³n y ConfiguraciÃ³n](#ï¸-instalaciÃ³n-y-configuraciÃ³n)
  - [1ï¸âƒ£ Clonar el repositorio](#1ï¸âƒ£-clonar-el-repositorio)
  - [2ï¸âƒ£ Crear Entorno Virtual](#2ï¸âƒ£-crear-entorno-virtual)
  - [3ï¸âƒ£ Instalar Dependencias](#3ï¸âƒ£-instalar-dependencias)
  - [4ï¸âƒ£ Configurar Kernel de Jupyter (para Sprint 2)](#4ï¸âƒ£-configurar-kernel-de-jupyter-para-sprint-2)
  - [5ï¸âƒ£ Descargar y Ubicar Datasets](#5ï¸âƒ£-descargar-y-ubicar-datasets)
  - [6ï¸âƒ£ Entrenar Modelos (obligatorio antes de Sprint 3 y 4)](#6ï¸âƒ£-entrenar-modelos-obligatorio-antes-de-sprint-3-y-4)
- [ğŸ§ª Sprint 2: AnÃ¡lisis y ClusterizaciÃ³n](#-sprint-2-anÃ¡lisis-y-clusterizaciÃ³n)
  - [ğŸ¯ Objetivos](#-objetivos)
  - [ğŸ› ï¸ TecnologÃ­as Utilizadas](#ï¸-tecnologÃ­as-utilizadas)
  - [ğŸ“Š EjecuciÃ³n del Notebook](#-ejecuciÃ³n-del-notebook)
  - [ğŸ“ˆ MÃ³dulos Disponibles](#-mÃ³dulos-disponibles)
    - [ğŸ”¹ Clustering](#-clustering)
    - [ğŸ”¹ Series Temporales](#-series-temporales)
    - [ğŸ”¹ RNN desde Cero en NumPy](#-rnn-desde-cero-en-numpy)
  - [ğŸ“Š Resultados Clave del Sprint 2](#-resultados-clave-del-sprint-2)
- [ğŸŒ Sprint 3: API de Recomendaciones](#-sprint-3-api-de-recomendaciones)
  - [ğŸ¯ Objetivos](#-objetivos-1)
  - [ğŸ› ï¸ TecnologÃ­as Utilizadas](#ï¸-tecnologÃ­as-utilizadas-1)
  - [ğŸš€ Iniciar la API](#-iniciar-la-api)
  - [ğŸ“š DocumentaciÃ³n Interactiva](#-documentaciÃ³n-interactiva)
  - [ğŸ”Œ Endpoints Disponibles](#-endpoints-disponibles)
  - [ğŸ§  LÃ³gica de Recomendaciones](#-lÃ³gica-de-recomendaciones)
  - [ğŸ“Š Resultados Clave del Sprint 3](#-resultados-clave-del-sprint-3)
- [ğŸ“Š Sprint 4: Dashboard Interactivo](#-sprint-4-dashboard-interactivo)
  - [ğŸ¯ Objetivos](#-objetivos-2)
  - [ğŸ› ï¸ TecnologÃ­as Utilizadas](#ï¸-tecnologÃ­as-utilizadas-2)
  - [ğŸš€ Iniciar el Dashboard](#-iniciar-el-dashboard)
  - [ğŸ¨ Funciones del Dashboard](#-funciones-del-dashboard)
    - [1ï¸âƒ£ PredicciÃ³n Individual](#1ï¸âƒ£-predicciÃ³n-individual)
    - [2ï¸âƒ£ GrÃ¡fico Radar Comparativo](#2ï¸âƒ£-grÃ¡fico-radar-comparativo)
    - [3ï¸âƒ£ EstadÃ­sticas Globales](#3ï¸âƒ£-estadÃ­sticas-globales)
    - [4ï¸âƒ£ Historial Completo](#4ï¸âƒ£-historial-completo)
    - [5ï¸âƒ£ BÃºsqueda por Estudiante](#5ï¸âƒ£-bÃºsqueda-por-estudiante)
    - [6ï¸âƒ£ Limpieza de Historial (`/clear-history`)](#6ï¸âƒ£-limpieza-de-historial-clear-history)
    - [7ï¸âƒ£ Estado de la API](#7ï¸âƒ£-estado-de-la-api)
- [ğŸ”„ Flujo de Trabajo Completo](#-flujo-de-trabajo-completo)
- [ğŸ§± Pipeline Completo Paso a Paso](#-pipeline-completo-paso-a-paso)
- [ğŸ“ˆ Resultados y Conclusiones](#-resultados-y-conclusiones)
  - [ğŸ”¹ Resultados TÃ©cnicos](#-resultados-tÃ©cnicos)
  - [ğŸ”¹ Conclusiones AcadÃ©micas](#-conclusiones-acadÃ©micas)
  - [ğŸ”¹ Hallazgos Principales](#-hallazgos-principales)

---

# ğŸ¯ **DescripciÃ³n del Proyecto**
Este proyecto implementa un sistema integral capaz de:

- ğŸ” Analizar datos educativos del ICFES  
- ğŸ§  Aplicar tÃ©cnicas de **clustering** para descubrir perfiles estudiantiles  
- ğŸ“ˆ Modelar series temporales (ARIMA y RNN)  
- ğŸ¤– Generar predicciones automÃ¡ticas  
- ğŸ“ Recomendar carreras y Ã¡reas de refuerzo  
- ğŸ“Š Visualizar resultados mediante un **dashboard interactivo**  

Todo organizado en 3 sprints:  
| Sprint | Objetivo | TecnologÃ­as |
|--------|----------|-------------|
| **Sprint 2** | AnÃ¡lisis y clustering | Scikit-learn, Statsmodels, NumPy |
| **Sprint 3** | API REST para predicciones | FastAPI, Uvicorn, Pickle |
| **Sprint 4** | Dashboard interactivo | Streamlit, Plotly |

---

# ğŸ“‚ **Datasets**
### ğŸ“Œ **Fuente**
Datos reales del ICFES:
- **Saber 11 â€“ 2020-2**
- **Saber Pro â€“ 2021 a 2024**

### ğŸ“¥ Descarga
ğŸ”— *Enlace a Google Drive (datasets limpios)*  
*(https://drive.google.com/drive/folders/1O49JVxhRDbB1oaLek9JYvX1UWl59MEmo)*

### ğŸ“ UbicaciÃ³n esperada
data/
â”œâ”€â”€ Dataset1â€“Saber11(2020-2)_LIMPIO.csv
â””â”€â”€ Dataset2â€“SaberPro(2021â€“2024)_LIMPIO.csv

âš ï¸ Nota importante: Los datasets NO estÃ¡n incluidos en el repositorio debido a su tamaÃ±o (>100 MB). El archivo .gitignore excluye automÃ¡ticamente *.csv y la carpeta data/.


---

# ğŸ—ï¸ **Arquitectura del Proyecto**

Proyecto/
â”‚
â”œâ”€â”€ src/icfes_analytics/          # ğŸ“¦ MÃ³dulos analÃ­ticos (Sprint 2)
â”‚   â”œâ”€â”€ clustering.py             # Algoritmos de clustering
â”‚   â”œâ”€â”€ timeseries.py             # AnÃ¡lisis de series temporales
â”‚   â”œâ”€â”€ rnn_numpy.py              # RNN implementada en NumPy puro
â”‚   â”œâ”€â”€ plots.py                  # Utilidades de visualizaciÃ³n
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ api/app/                      # ğŸŒ API REST (Sprint 3)
â”‚   â”œâ”€â”€ main.py                   # Endpoints de FastAPI
â”‚   â”œâ”€â”€ services.py               # LÃ³gica de negocio
â”‚   â”œâ”€â”€ schemas.py                # Modelos Pydantic
â”‚   â””â”€â”€ models_loader.py          # Carga de modelos ML
â”‚
â”œâ”€â”€ dashboard/                    # ğŸ“Š Dashboard (Sprint 4)
â”‚   â””â”€â”€ app.py                    # AplicaciÃ³n Streamlit
â”‚
â”œâ”€â”€ models/                       # ğŸ¤– Modelos entrenados
â”‚   â”œâ”€â”€ scaler.pkl                # StandardScaler ajustado
â”‚   â”œâ”€â”€ kmeans.pkl                # Modelo K-Means
â”‚   â””â”€â”€ feature_cols.pkl          # Lista de features
â”‚
â”œâ”€â”€ data/                         # ğŸ“ Datasets (no incluidos en repo)
â”‚   â”œâ”€â”€ Dataset1â€“Saber11(2020-2)_LIMPIO.csv
â”‚   â””â”€â”€ Dataset2â€“SaberPro(2021â€“2024)_LIMPIO.csv
â”‚
â”œâ”€â”€ Sprint2_ICFES.ipynb           # ğŸ““ Notebook principal (Sprint 2)
â”œâ”€â”€ train_save_models.py          # ğŸ“ Script de entrenamiento
â”œâ”€â”€ requirements.txt              # ğŸ“‹ Dependencias unificadas
â””â”€â”€ README.md                     # ğŸ“– Este archivo



---

# ğŸ§° **Requisitos Previos**
### âœ”ï¸ Software necesario
- Python **3.10 â€“ 3.12**
- pip actualizado
- Git
- Windows / Linux / macOS

### âœ”ï¸ Verificar versiones
```bash
python --version
pip --version
```


# âš™ï¸ InstalaciÃ³n y ConfiguraciÃ³n
## 1ï¸âƒ£ Clonar el repositorio
```
git clone https://github.com/Andres-Nieto/Sprint2-Proyecto-Inteligencia-de-Negocios.git
cd Sprint2-Proyecto-Inteligencia-de-Negocios
```

## 2ï¸âƒ£ Crear Entorno Virtual
Windows (PowerShell):
```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```
Linux/macOS:
```bash
python3 -m venv .venv
source .venv/bin/activate
```

## 3ï¸âƒ£ Instalar Dependencias
```
python -m pip install --upgrade pip
pip install -r requirements.txt
```

## 4ï¸âƒ£ Configurar Kernel de Jupyter (para Sprint 2)
```
python -m ipykernel install --user --name icfes-analytics
```

## 5ï¸âƒ£ Descargar y Ubicar Datasets
- Descarga los CSV desde Google Drive
- Crea la carpeta data/ en la raÃ­z del proyecto
- Copia los archivos CSV dentro de data/

## 6ï¸âƒ£ Entrenar Modelos (obligatorio antes de Sprint 3 y 4)
```
python train_save_models.py
```

Esto generarÃ¡ los archivos en models/:
- scaler.pkl - Normalizador de features
- kmeans.pkl - Modelo de clustering
- feature_cols.pkl - Lista de columnas utilizadas

# ğŸ§ª Sprint 2: AnÃ¡lisis y ClusterizaciÃ³n
## ğŸ¯ Objetivos
- Aplicar tÃ©cnicas de clustering (particional, jerÃ¡rquico y por densidad).
- Analizar series temporales de puntajes.
- Implementar modelos de pronÃ³stico: ARIMA y RNN desde cero con NumPy.

## ğŸ› ï¸ TecnologÃ­as Utilizadas
- **Clustering:** K-Means, DBSCAN, Hierarchical Clustering (Scikit-learn)
- **Series temporales:** STL Decomposition, Test ADF, ARIMA (Statsmodels)
- **Deep learning:** RNN implementada manualmente con NumPy
- **VisualizaciÃ³n:** Matplotlib, Seaborn

## ğŸ“Š EjecuciÃ³n del Notebook
1. Abrir VS Code en la carpeta del proyecto.
2. Abrir el archivo: `Sprint2_ICFES.ipynb`
3. Seleccionar kernel: **icfes-analytics**
4. Ejecutar todas las celdas en orden.

## ğŸ“ˆ MÃ³dulos Disponibles

### ğŸ”¹ Clustering
```python
from icfes_analytics.clustering import run_six_clustering_plots

X_scaled  # array con features estandarizadas

resumen = run_six_clustering_plots(X_scaled, n_clusters=5)
print(resumen)
```

Genera automÃ¡ticamente los siguientes grÃ¡ficos:
- K-Means  
- DBSCAN  
- Hierarchical Clustering (Ward)  
- Silhouette Analysis  
- Elbow Method  
- Dendrograma  

---

### ğŸ”¹ Series Temporales
```python
from icfes_analytics.timeseries import (
    aggregate_series,
    fit_arima_small_grid,
    plot_arima_forecast
)

# Agregar datos por periodo
agg = aggregate_series(df, period_col='periodo', value_col='punt_global')

# Ajustar modelo ARIMA
order, result, y_pred, y_true, train, test, metrics = fit_arima_small_grid(agg)
print(f"Mejor modelo ARIMA{order}: RMSE={metrics['rmse']:.2f}")

# Visualizar pronÃ³stico
plot_arima_forecast(train, test, y_pred, order)
```

---

### ğŸ”¹ RNN desde Cero en NumPy
```python
from icfes_analytics.rnn_numpy import forecast_one_step_numpy

y_pred, y_true, metrics = forecast_one_step_numpy(
    agg,
    freq='QS-MAR',
    window=4,
    hidden_size=16,
    epochs=600
)

print(f"RNN Metrics: {metrics}")
```

---

## ğŸ“Š Resultados Clave del Sprint 2
- IdentificaciÃ³n de **5 perfiles acadÃ©micos** mediante clustering.
- ARIMA alcanzÃ³ un **RMSE â‰ˆ 15 puntos**.
- RNN logrÃ³ capturar **patrones no lineales**.
- Se generaron **6 grÃ¡ficos automÃ¡ticos** de anÃ¡lisis.

---

# ğŸŒ Sprint 3: API de Recomendaciones

## ğŸ¯ Objetivos
- Construir API REST para predicciones en tiempo real.  
- Crear sistema de recomendaciones acadÃ©micas.  
- Mantener historial de consultas en memoria.

## ğŸ› ï¸ TecnologÃ­as Utilizadas
- FastAPI  
- Uvicorn  
- Pydantic  
- Pickle  

## ğŸš€ Iniciar la API
```bash
uvicorn api.app.main:app --reload
```

URL base:  
http://127.0.0.1:8000

## ğŸ“š DocumentaciÃ³n Interactiva
- **Swagger UI:** `/docs`
- **ReDoc:** `/redoc`

## ğŸ”Œ Endpoints Disponibles
| Endpoint | MÃ©todo | DescripciÃ³n |
|---------|--------|-------------|
| `/health` | GET | Estado de la API |
| `/predict` | POST | Predice clÃºster y recomendaciones |
| `/cluster` | POST | Igual que predict + guarda historial |
| `/history` | GET | Ãšltimas 50 consultas |
| `/summary` | GET | Resumen por clÃºster |
| `/student/{id}` | GET | Historial por estudiante |
| `/clear-history` | DELETE | Limpia memoria |

## ğŸ§  LÃ³gica de Recomendaciones
Basada en:  
- ClÃºster asignado  
- Fortaleza principal  
- Carreras sugeridas segÃºn perfil  
- Ãreas de refuerzo  

## ğŸ“Š Resultados Clave del Sprint 3
- API con **7 endpoints funcionales**
- Tiempo de respuesta **< 50 ms**
- DocumentaciÃ³n automÃ¡tica
- Arquitectura lista para despliegue en la nube

---

# ğŸ“Š Sprint 4: Dashboard Interactivo

## ğŸ¯ Objetivos
- Crear interfaz visual para usuarios finales  
- Integrar API del Sprint 3  
- Mostrar estadÃ­sticas y comparativas en tiempo real  

## ğŸ› ï¸ TecnologÃ­as Utilizadas
- Streamlit  
- Plotly  
- Requests  

## ğŸš€ Iniciar el Dashboard
```bash
# Encender la API
uvicorn api.app.main:app --reload

# Encender el dashboard
streamlit run dashboard/app.py
```

Disponible en: http://localhost:8501

---

## ğŸ¨ Funciones del Dashboard

### 1ï¸âƒ£ PredicciÃ³n Individual
Muestra:
- ClÃºster asignado  
- Fortaleza principal  
- Carreras sugeridas  
- Ãreas de refuerzo  

### 2ï¸âƒ£ GrÃ¡fico Radar Comparativo
Compara:
- Estudiante (azul)
- Promedio nacional (rojo)

### 3ï¸âƒ£ EstadÃ­sticas Globales
- Promedios por Ã¡rea  
- Total de consultas  
- Filtros por rango de puntaje  

### 4ï¸âƒ£ Historial Completo  
- 50 Ãºltimas consultas  

### 5ï¸âƒ£ BÃºsqueda por Estudiante  

### 6ï¸âƒ£ Limpieza de Historial (`/clear-history`)

### 7ï¸âƒ£ Estado de la API  

---

# ğŸ”„ Flujo de Trabajo Completo
```
SPRINT 2 â†’ Clustering + Series Temporales
        â†“
train_save_models.py â†’ Entrena y guarda modelos
        â†“
SPRINT 3 â†’ API con 7 endpoints
        â†“
SPRINT 4 â†’ Dashboard conectado a la API
```

---

# ğŸ§± Pipeline Completo Paso a Paso

1. **PreparaciÃ³n de Datos**
2. **Clustering y series temporales (Sprint 2)**
3. **Entrenamiento:**
```bash
python train_save_models.py
```
4. **API:**
```bash
uvicorn api.app.main:app --reload
```
5. **Dashboard:**
```bash
streamlit run dashboard/app.py
```

---

# ğŸ“ˆ Resultados y Conclusiones

## ğŸ”¹ Resultados TÃ©cnicos
| MÃ©trica | Valor |
|--------|-------|
| Modelos clustering | K-Means, DBSCAN, JerÃ¡rquico |
| NÃºmero clÃºsteres | 5 |
| RMSE ARIMA | ~15 |
| Endpoints API | 7 |
| Tiempo API | < 50 ms |
| Visualizaciones | 5 tipos |

## ğŸ”¹ Conclusiones AcadÃ©micas
- Clustering revelÃ³ **5 perfiles acadÃ©micos**  
- ARIMA Ãºtil para cortoplazo  
- RNN Ãºtil para patrones no lineales  
- Arquitectura escalable  
- Dashboard accesible y claro  

## ğŸ”¹ Hallazgos Principales
- Perfil STEM  
- Perfil HumanÃ­stico  
- Perfil Balanceado  
- Perfil en Desarrollo  
- Perfil BilingÃ¼e  